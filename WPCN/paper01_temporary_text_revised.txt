Deep Learning-based Optimal Placement of a Mobile HAP for Wireless Powered Communication Networks
Hong-Sik Kim, Hanyang University, Department of Computer Software

Abstract
HAP (Hybrid Access Point) is a node in Wireless Powered Communication Networks (WPCN) that can distribute energy to each wireless device and also can receive information from these devices. There needs mobile HAPs for efficient network use and the throughput of the network depends on the location of HAPs, so we should maximize the throughput of the network. There are two kinds of metrics for throughput, that is, sum throughput and common throughput, each is the sum and minimum value of throughput between a HAP and each wireless device, respectively. There are two types of throughput maximization problems, sum throughput maximization and common throughput maximization. In this paper, we discuss about the latter. We proposed deep learning-based methodology to maximize common throughput by optimally placing a mobile HAP for WPCN. Our study implies that deep learning can be applied to optimize a simple function of common throughput maximization, which is a convex function or a combination of a few convex functions, and shows better performance than mathematical methodologies.

 
1. Introduction
In WPCN, there is Access Point (AP) mechanism [1] which contains energy nodes (EN), wireless devices (WD) and access points (AP). First, energy nodes send energy to each wireless device. When ENs receive the energy, it sends information to APs using the energy. That is, ENs send energy to WDs and WDs send information to APs. We can encapsulate AP and EN into Hybrid Access Point (HAP) and so can describe HAP mechanism. In this mechanism, the HAP sends energy to each WD, and each WD sends information to the HAP. The HAP allocates time for sending energy to each WD and itself, and for sending information to each WD, so time allocation for itself and each WD is also an important issue.
Because the distance between the HAP and each WD is different among each WD, there is an energy efficiency gap between the WDs caused by the difference of throughput for each WD. That is, a WD near to the HAP receives more energy and uses less energy to transmit information, and another WD far from the HAP receives less energy but uses more energy to transmit information. To solve this unfairness problem, the worst case, a WD which receives the least energy and uses the most energy, is very important. In this case, we use the concept of common throughput which is the minimum value of throughput among the throughput values of each WD, and we concentrate on maximizing the common throughput value in the WPCN environment.
This paper introduces a methodology to place HAPs in WPCN environment to maximize common throughput when time allocation is optimized, by using deep learning, and shows that this methodology is meaningful to solve this problem and shows better performance than the mathematical methodology already studied.
In [2], Suzhi Bi and Rui Zhang researched about the placement optimization of Energy and Information Access Point in WPCN using Bi-section search method, Greedy algorithm, Trial-and-error method, and alternating method for joint AP-EN placement. There can be more than 1 HAPs WDs in the supposed environment of this paper. Its methodology repeatedly adds HAPs and check if each WD satisfies conditions in the environment.

2. HAP Placement Model
2.1. Overview
Figure 1 is the flow chart for the HAP placement model. The model is composed of three phases. First, making data is to create training and test data. Next, training using data is to process the data to convert to training and test data for the deep learning model, and train using the model. Last, finding the best point is to find the best HAP placement point using the throughput map derived from this model.
 
 
Figure 2 describes the system architecture of the model. Mobile HAP can be placed at any place in the environment and can move to any other place in the environment. The goal is to maximize common throughput that is as same as the minimum throughput between the HAP and each WD by optimizing the HAP placement. WDs placement map means the square map with N rows and M columns, and K blocks in the map, randomly set at the training stage, contains a WD. There are no WDs with the same position in the map. From now on, we will call this WDPM_i (N,M,K). Throughput map means the square map with N rows and M columns, and each block contains the throughput value where the HAP is located in this block of WDPM_i (N,M,K). From now on, we will call this TM_i (N,M). Best throughput point means the position of HAP that maximizes throughput value in TM_i (N,M). We will call this BTP_i (N,M).

2.2. Making Data
To make WDPM_i (N,M,K)(0≤i<m)s where m is the sum of the number of training and test maps, first define a square map with N rows and M columns. Then repeat placing a WD at the randomly selected point K times. To make TM_i (N,M)(0≤i<m)s using these WDPM_i (N,M,K)s, place HAP at each point in WDPM_i (N,M,K)s and compute throughput for the location of HAP and each WD using Algorithm 1 because the throughput value is computed using Formula (6) in [3]. getThrput function finds optimal time allocation given WDPM_i (N,M,K). Because we supposed that ζ=1.0,h_i=0.001p_i^2 d^(-α_d )  where α_d=2.0,g_i=0.001p_i^2 d^(-α_u )  where α_u=2.0,p_i=1.0,P_A=20.0,Γ=9.8 and σ=0.001 where d is the distance from the HAP and each WD, this formula can be converted into (1). To prevent divide by 0 error and consider the limit of throughput, we supposed that distance is 1.0 when actual distance is less than 1.0. 
R_i (τ)=τ_i  log_2⁡(1+(100p_i^4)/(49∙〖max⁡(d,1)〗^4 )  τ_0/τ_i ),i=1,…,K,(1)
Then, because WDPM_i (N,M,K)s and TM_i (N,M)s are saved as text files, the model must read them before using them.

2.3. Training
First, make input data for training and testing based on WDPM_i (N,M,K)(0≤i<m_1+m_2), supposing that the number of training and testing data is m_1 and m_2 each. The model considers first m_1 maps as training data and next m_2 maps as test data. The input data is an N×M map whose value at each block is -1 when a WD is at this block and 0 otherwise. Then make output data for training based on TM_i (N,M)(0≤i<m_1)s corresponding to WDPM_i (N,M,K)(0≤i<m_1)s. 
 
The output data is an N×M map whose value at each block is processed version of the common throughput value derived by Algorithm 1 where the HAP is on this block. The following is the processing procedure. First, find maximum throughput value for each training output map i (0≤i<m_1), and then divide each value V_nm^i  (0≤i<m_i,0≤n<N,0≤m<M), at the intersection of row n and column m on this map by the maximum value among these values. Last, process each value V_nm^i at each block using (2).
V_nm^i'=Sigmoid(2V_nm-1)  where Sigmoid(x)=1/(1+e^(-x) )    (2)
Then train using m_1 input data WDPM_i (N,M,K)(0≤i<m_1)s and corresponding m_1 output data TM_i (N,M)(0≤i<m_1)s using the deep learning model described in Figure 3 with Adam optimizer [4] with learning rate 0.0001 and 1,000 epochs.

2.4. Finding the Best Point
Using test input data, the model finds best point for HAP placement. For each test input data created based on DPM_i (N,M,K)(m_1≤i<m_1+m_2), input these data into the model trained in 2.3. and get output maps corresponding to TM_i (N,M)(m_1≤i<m_1+m_2). For each value V_nm^i' at each block in each output map is converted by (3) using the inverse function of the sigmoid function, to convert them from V_nm^i'=Sigmoid(2V_nm^i-1) from into V_nm^i''=2V_nm^i-1 form, where V_nm^i is the estimated common throughput value.
V_nm^i''=invSigmoid(V_nm^i' )  where invSigmoid(x)=ln⁡〖x/(1-x)〗    (3)
Then, for each output map, the model finds the maximum value among values in blocks of this map. Let’s call row and column axis of this value in the map n_Max and m_Max each, and call the maximum value V_(n_Max m_Max)^i''. Then the row axis n_optimal and column axis m_optimal of optimal HAP location are computed by (4) and (5) each, and BTP_i (N,M) is computed by (6), described in Figure 4.
 
n_optimal=n_Max+(V_(n_Max+1,m_Max)^i''-V_(n_Max-1,m_Max)^i'')/(V_(n_Max-1,m_Max)^i''+V_(n_Max,m_Max)^i''+V_(n_Max+1,m_Max)^i'' )    (4)
m_optimal=m_Max+(V_(n_Max,m_Max+1)^i''-V_(n_Max,m_Max-1)^i'')/(V_(n_Max,m_Max-1)^i''+V_(n_Max,m_Max)^i''+V_(n_Max,m_Max+1)^i'' )    (5)
BTP_i (N,M)=(n_optimal,m_optimal )    (6)
If V_(n_Max+1,m_Max)^i'' is greater than V_(n_Max-1,m_Max)^i'', n_optimal moves down from original position and otherwise it moves up. Similarly, if V_(n_Max,m_Max+1)^i'' is greater than V_(n_Max,m_Max-1)^i'', m_optimal moves right and otherwise it moves left. Because original common throughput V_nm^i and 2V_nm^i-1 can be converted into each other by just a linear transmission, there is no difference of n_optimal and m_optimal between when converted V_nm^i'' into V_nm^i and do not convert V_nm^i'' into any other form.

3. Simulation Model
3.1. Experiment Design and Test Metrics
Figure 5 is the flow chart for the experiment. For each optimal HAP location for each test map BTP_i (N,M)=(n_optimal,m_optimal )(m_1≤i<m_1+m_2) derived by 2.4 corresponding to TM_i (N,M)(m_1≤i<m_1+m_2)s, first compute common throughput value C_i (m_1≤i<m_1+m_2) using this point. Because we use TM_i (N,M)(m_1≤i<m_1+m_2)s only for computing the difference when testing, the throughput maps as created as the output of the model, called TM_i^' (N,M)(m_1≤i<m_1+m_2)s in this section, are not equal to corresponding TM_i (N,M)(m_1≤i<m_1+m_2)s. Then compare the throughput value with the maximum common throughput value 〖MC〗_i (m_1≤i<m_1+m_2) in corresponding TM_i (N,M). Then the test metrics are defined as and computed using (7), (8) and (9).
 
CT.AVERAGE=(∑_(i=m_1)^(m_1+m_2-1)▒C_i )/n    (7)
CT.AVGMAX=(∑_(i=m_1)^(〖m_1+m〗_2-1)▒〖MC_i 〗)/n    (8)
CT.RATE=(∑_(i=m_1)^(〖m_1+m〗_2-1)▒C_i )/(∑_(i=m_1)^(〖m_1+m〗_2-1)▒〖MC_i 〗)=(CT.AVERAGE)/(CT.AVGMAX)    (9)
CT.AVERAGE means average common throughput for each test map, CT.AVGMAX means maximum common throughput value for each throughput map corresponding to each test map, and CT.RATE means the rate between the sum of C_i and the sum of MC_i for all test maps. It also means the rate between CT.AVERAGE and CT.AVGMAX. We also define performance rate PR as (10) meaning how well our methodology is compared to the methodology used in the original paper, and the original paper in (10) means [2].
PR=((CT.AVERAGE of our methodology))/((CT.AVERAGE of methodology in original paper) )    (10)

3.2. Experiment environment
The computer system information for our experiment is as the following. The operating system is Window 10 Pro 64bit (10.0, build 18363), system manufacturer is LG Electronics, the system model is 17ZD90N-VX5BK, the BIOS is C2ZE0160 X64, the processor is Intel® Core™ i5-1035G7 CPU @ 1.20GHz (8 CPUs), ~1.5GHz, and the memory is 16384MB RAM. The programming language is Python 3.7.4, and used NumPy, Tensorflow and Keras as libraries. You can download the experiment code written in Python from https://github.com/WannaBeSuperteur/2020/tree/master/WPCN.

3.3. Experimental Results
Figure 6 describes CT.RATE (%) and CT.AVERAGE values for our methodology and the methodology in the original paper. For our methodology, CT.RATE value increases when the number of WDs increases and decreases when the size of maps increases, and CT.AVERAGE decreases when both the number of WDs and the size of maps increases. For the methodology in the original paper, CT.RATE increases when the size of maps increases, but has no significant correlation with the number of WDs, and as the same as our methodology, CT.AVERAGE decreases when both the number of WDs and the size of maps increases. Figure 7 describes CT.AVGMAX and PR values for each size and number of WDs. CT.AVGMAX decreases when both the number of WDs and the size of maps increases and PR decreases when the size of maps increases, but has no significant correlation with the number of WDs. For small size, our methodology shows significantly better performance than the methodology in the original paper, but for 16x16 size, these two methods show almost the same performance. Figure 8 is the line chart representation of Figure 6 and Figure 7.

4. Conclusion
We showed that our method using deep learning shows better performance than the mathematical method in the original paper [2]. Although our method shows better performance when the size is smaller and may show worse performance than the method in the original paper if the size is larger than 16x16, our approach to find the optimal placement and time allocation for HAP using deep learning is meaningful because there is no attempt to apply deep learning to this problem yet. There are some limits for our study. First, our study has an advantageous point for our method that it uses only 1 HAP but the method in the original paper may and commonly uses more than 1 HAPs. Second, we studied with just a few conditions, 3 options for map size and 2 options for the number of WDs. So some future study should be done for many options for map size and the number of WDs, and additionally the number of HAPs.

5. References
[1] Suzhi Bi, Yong Zeng, and Rui Zhang, “Wireless Powered Communication Networks: An Overview”, IEEE, available online at https://arXiv:1508.06366
[2] Suzhi Bi, Member, IEEE, and Rui Zhang, “Placement Optimization of Energy and Information Access Points in Wireless Powered Communication Networks”, IEEE Transactions on wireless communications, VOL. 15, NO. 3, MARCH 2016
[3] Hyungsik Ju and Rui Zhang, “Throughput Maximization in Wireless Powered Communication Networks”, available online at https://arXiv:1304.7886v4
 
[4] Diederik P. Kingma, Jimmy Lei Ba, “ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION”, ICLR 2015, available online at https://arXiv:1412.6980

