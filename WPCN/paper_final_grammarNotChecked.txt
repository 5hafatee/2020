Deep Learning-based Optimal Placement of a Mobile HAP for Wireless Powered Communication Networks
Hong-Sik Kim and In-Whee Joe, Hanyang University, Department of Computer Software

 
Abstract—HAP (Hybrid Access Point) is a node in Wireless Powered Communication Networks (WPCN) that can distribute energy to each wireless device and also can receive information from these devices. There needs mobile HAPs for efficient network use and the throughput of the network depends on the location of HAPs, so we should maximize the throughput of the network. There are two kinds of metrics for throughput, that is, sum throughput and common throughput, each is the sum and minimum value of throughput between a HAP and each wireless device, respectively. There are two types of throughput maximization problems, sum throughput maximization and common throughput maximization. In this paper, we discuss about the latter. We propose deep learning-based methodology to maximize common throughput by optimally placing a mobile HAP for WPCN. Our study implies that deep learning can be applied to optimize a simple function of common throughput maximization, which is a convex function or a combination of a few convex functions, and shows better performance than mathematical methodologies.

INTRODUCTION
I
N WPCN, there is Access Point (AP) mechanism [1] which contains energy nodes (EN), wireless devices (WD) and access points (AP). First, energy nodes send energy to each wireless device. When ENs receive the energy, it sends information to APs using the energy. That is, ENs send energy to WDs and WDs send information to APs. We can encapsulate AP and EN into Hybrid Access Point (HAP) and so can describe HAP mechanism. In this mechanism, the HAP sends energy to each WD, and each WD sends information to the HAP. The HAP allocates time for sending energy to each WD and itself, and for sending information to each WD, so time allocation for itself and each WD is also an important issue.
Because the distance between the HAP and each WD is different among each WD, there is an energy efficiency gap between the WDs caused by the difference of throughput for each WD. That is, a WD near to the HAP receives more energy and uses less energy to transmit information, and another WD far from the HAP receives less energy but uses more energy to transmit information. To solve this unfairness problem, the worst case, a WD which receives the least energy and uses the most energy, is very important. In this case, we use the concept of common throughput which is the minimum value of throughput among the throughput values of each WD, and we concentrate on maximizing the common throughput value in the WPCN environment.
This paper introduces a methodology to place HAPs in WPCN environment to maximize common throughput when time allocation is optimized, by using deep learning, and shows that this methodology is meaningful to solve this problem and shows better performance than the mathematical methodology already studied.
In [2], Suzhi Bi and Rui Zhang researched about the placement optimization of Energy and Information Access Point in WPCN using bisection search method, Greedy algorithm, Trial-and-error method, and alternating method for joint AP-EN placement. There can be more than 1 HAPs WDs in the supposed environment of this paper. Its methodology repeatedly adds HAPs and check if each WD satisfies conditions in the environment.
HAP PLACEMENT MODEL
Overview
Fig. 1. is the flow chart of the HAP placement model. The model is composed of three phases. First, making data is to create training and test data. Next, training using data is to process the data to convert to training and test data for the deep learning model, and train using the model. Last, finding the best point is to find the best HAP placement point using the throughput map derived from this model.
Fig. 2. describes the system architecture of the model. Mobile HAP can be placed at any place in the environment and can move to any other place in the environment. The goal is to maximize common throughput that is defined as the minimum throughput 
 
Fig. 1. Flow chart of HAP placement model. Note that the number after the description of each phase means the order of the stage whose data is used in this stage.

 
Fig. 2. The system architecture of our model. Wireless devices are placed in the environment and we can represent the location of WDs as a WDs placement map. A mobile HAP is placed in the environment and the throughput value for the environment is calculated and represented as a throughput map.

between the HAP and each WD by optimizing the HAP placement. WDs placement map means the square map with N rows and M columns, and K blocks in the map, randomly set at the training stage, contains a WD. There are no WDs with the same position in the map. From now on, we will call this WDPM_i (N,M,K). Throughput map means the square map with N rows and M columns, and each block contains the throughput value where the HAP is located in this block of WDPM_i (N,M,K). From now on, we will call this TM_i (N,M). Best throughput point means the position of HAP that maximizes throughput value in TM_i (N,M), derived from our model, so it could be not a real position that maximizes the throughput value. We will call this BTP_i (N,M).
Making Data
To make WDPM_i (N,M,K)(0≤i<m_total)s where m_total is the sum of the number of training and test maps, first define a square map with N rows and M columns. Then repeat placing a WD at the randomly selected point K times. To make TM_i (N,M)(0≤i<m_total)s using these WDPM_i (N,M,K)s, place HAP at each point in WDPM_i (N,M,K)s and compute throughput for the location of HAP and each WD using Algorithm 1 because the throughput is computed using Formula (6)
Algorithm 1. Finding throughput for a HAP location and each WD’s location
input: list of the location of each WD wdList, location (y-axis and x-axis) of HAP HAPpoint
output: throughput value finalThrput
lr ← 5.0;
timeList ← [1.0, 1.0, …, 1.0];
for i = 1 To 1000 do
tpChange ← [];
thrput ← getThrput(wdList, HAPpoint, timeList);
for each x in {HAP, wdList} do
timeListCopy ← timeList;
timeListCopy[x] ← timeListCopy[x] + 1;
newThrput ← getThrput(wdList, HAPpoint, timeListCopy);
difThrput ← log_2⁡〖(newThrput/thrput)〗;
append max(0.01*log_2⁡〖(difThrput)〗, log_2⁡〖(difThrput)〗) to tpChange;
timeList[x] ← timeList[x] * 2^(lr*tpChange[x]);
end
end
finalThrput ← getThrput(wdList, HAPpoint, timeList);
Return finalThrput;
getThrput(wdList, HAPpoint, timeList):
(〖HAP〗_y, 〖HAP〗_x) ← (Y axis of HAPpoint, X axis of HAPpoint);
sumOfTime ← sum(timeList);
HAPtime ← timeList[0]/sumOfTime;
result ← INFINITE;
for each WD i do
(〖WD〗_y, 〖WD〗_x) ← (Y axis of wdList[i], X axis of wdList[i]);
dist ← √((WD_y-HAP_y )^2+(WD_x-HAP_x )^2 );
chargeTime ← timeList[1+i]/sumOfTime;
throughput ← chargeTime *log_2⁡(1+((100p_i^4 ))/(49∙(max⁡(dist,1) )^4 )∙HAPtime/chargetime);
result ← min(result, throughput);
end
Return result;

in [3]. getThrput function finds optimal time allocation given WDPM_i (N,M,K). Because we supposed that ζ=1.0,h_i=0.001p_i^2 d^(-α_d )  where α_d=2.0,g_i=0.001p_i^2 d^(-α_u )  , α_u=2.0,p_i=1.0,P_A=20.0,Γ=9.8 and σ=0.001 where d is the distance from the HAP and each WD, this formula can be converted into (1). To prevent divide by 0 error and consider the limit of throughput, we supposed that distance is 1.0 when actual distance is less than 1.0.
	
R_i (τ)=τ_i  log_2⁡(1+(100p_i^4)/(49∙〖max⁡(d,1)〗^4 )  τ_0/τ_i ),
i=1,…,K	(1)

Then, because WDPM_i (N,M,K)s and TM_i (N,M)s are saved as text files, the model must read them before using them.
Training
First, make input data for training and testing based on WDPM_i (N,M,K)(0≤i<m_1+m_2), supposing that the number of training and testing data is m_1 and m_2 each. The model considers first m_1 maps as training data and next m_2 maps as test data. The input data is an N×M map whose value at each block is -1 when a WD is on this block and 0 otherwise. Then make output data for training based on TM_i (N,M)(0≤i<m_1)s corresponding to WDPM_i (N,M,K)(0≤i<m_1)s.
The output data is an N×M map whose value at each block is processed version of the common throughput value derived by Algorithm 1 where the HAP is on this block. We define V_nm^i  (0≤i<m_1,0≤n<N,0≤m<M) as the value on the block of n-th row and m-th column of the training output map corresponding to WDPM_i (N,M,K)  (0≤i<m_1). The following is the processing procedure. First, find maximum throughput value max⁡(V_nm^i )  (0≤n<N,0≤m<M) for each training output map i (0≤i<m_1), and then divide each value V_nm^i  (0≤i<m_1,0≤n<N,0≤m<M) by max⁡(V_nm^i ). Last, process each value V_nm^i at each block using (2).

V_nm^(i^' )=Sigmoid(2V_nm^i-1)
where Sigmoid(x)=1/(1+e^(-x) )	(2)

Then train using m_1 input data WDPM_i (N,M,K)(0≤i<m_1)s and corresponding m_1 output data TM_i (N,M)(0≤i<m_1)s using the deep learning model described in Fig. 3. with Adam optimizer [4] with learning rate 0.0001 and 1,000 epochs.
Finding the Best Points
Using test input data, the model finds best point for HAP placement. For each test input data created based on WDPM_i (N,M,K)(m_1≤i<m_1+m_2), input these data into the model trained in 2.3. and get output maps corresponding to TM_i (N,M)(m_1≤i<m_1+m_2). For each value V_nm^i' at each block in each output map is converted by (3) using the inverse function of the sigmoid function, to convert them from V_nm^i'=Sigmoid(2V_nm^i-1) from into V_nm^i''=2V_nm^i-1 form, where V_nm^i is the estimated common throughput value.

V_nm^(i^'' )=invSigmoid(V_nm^(i^' ) )
where invSigmoid(x)=ln⁡〖x/(1-x)〗	(3)

Then, for each output map, the model finds the maximum value among values in blocks of this map. Let’s call row and column axis of this value in the map n_Max and m_Max each, and call the maximum value V_(n_Max m_Max)^i''. Then the row axis n_optimal and column axis m_optimal of optimal HAP location are computed
 
Fig. 3. Architecture of Deep Learning Model for Common Throughput Maximization: We use Convolutional Neural Network (CNN) [5] for our methodology.

 
Fig. 4. Decision algorithm for n_optimal and m_optimal. For each picture in the left and right, n_Max-1, n_Max, n_Max+1, m_Max-1, m_Max, and m_Max+1 means the y and x axis of the environment, respectively. Each rectangle with the value means the common throughput value at the point of these y and x axis.

by (4) and (5) each, and BTP_i (N,M) is computed by (6), described in Fig. 4.

n_optimal=n_Max+
(V_(n_Max+1,m_Max)^i''-V_(n_Max-1,m_Max)^i'')/(V_(n_Max-1,m_Max)^i''+V_(n_Max,m_Max)^i''+V_(n_Max+1,m_Max)^i'' )
(4)
m_optimal=m_Max+
(V_(n_Max,m_Max+1)^i''-V_(n_Max,m_Max-1)^i'')/(V_(n_Max,m_Max-1)^i''+V_(n_Max,m_Max)^i''+V_(n_Max,m_Max+1)^i'' )
(5)
BTP_i (N,M)=(n_optimal,m_optimal )
(6)

If V_(n_Max+1,m_Max)^i'' is greater than V_(n_Max-1,m_Max)^i'', n_optimal moves down from original position and otherwise it moves up. Similarly, if V_(n_Max,m_Max+1)^i'' is greater than V_(n_Max,m_Max-1)^i'', m_optimal moves right and otherwise it moves left. Because original common throughput V_nm^i and 2V_nm^i-1 can be converted into each other by just a linear transmission, there is no difference of n_optimal and m_optimal between when converted V_nm^i'' into V_nm^i and do not convert V_nm^i'' into any other form.
SIMULATION MODEL
Experiment Design and Test Metrics
Fig. 5. is the flow chart for the experiment. For each optimal HAP location for each test map BTP_i (N,M)=(n_optimal,m_optimal )  (m_1≤i<m_1+m_2) derived by 2.4 corresponding to TM_i (N,M)(m_1≤i<m_1+m_2)s, first compute common throughput value C_i (m_1≤i<m_1+m_2) using this point. Because we use TM_i (N,M)(m_1≤i<m_1+m_2)s only for computing the difference when testing, the throughput maps as created as the output of the model, called TM_i^' (N,M)(m_1≤i<m_1+m_2)s in this section, are not equal to corresponding TM_i (N,M)(m_1≤i<m_1+m_2)s. Then compare the throughput value with 〖MC〗_i (m_1≤i<m_1+m_2), the maximum common throughput value among all points (n,m)(0≤n<N,0≤m<M,n and m are all integers) in corresponding TM_i (N,M). Then the test metrics are defined as and computed using (7), (8) and (9).

 
Fig. 5. Flow chart of design of testing. Note that 1-1 after the description of stage 1-2 means the data of stage 1-1 is used in stage 1-2.


Algorithm 2. Solving (20) in the original paper
input: λ_k,a_(1,k),a_(2,k),w_k,d_U and v_(1_k ), as defined in the original paper
output: optimal value of v_(1_k ) (by Gradient Descent Method)
lr ← 3×〖10〗^8;
for i = 1 To 7000 do
t_min ^(v_(1_k ) )← min(λ_k-a_(1,k)-a_(2,k) ‖v_(1_k )-w_k ‖^(d_U ) ), k = 1,…,K;
v_(1_k)^0← [v_(1_k ) [0]+〖10〗^(-6),v_(1_k ) [1]];
v_(1_k)^1← [v_(1_k ) [0],v_(1_k ) [1]+〖10〗^(-6) ];
t_min ^(v_(1_k)^0 )← min(λ_k-a_(1,k)-a_(2,k) ‖v_(1_k)^0-w_k ‖^(d_U ) ), k = 1,…,K;
t_min ^(v_(1_k)^1 )← min(λ_k-a_(1,k)-a_(2,k) ‖v_(1_k)^1-w_k ‖^(d_U ) ), k = 1,…,K;
v_(1_k ) [0]← v_(1_k ) [0]+ lr*(t_min ^(v_(1_k)^0 )-t_min ^(v_(1_k ) ) );
v_(1_k ) [1]←v_(1_k ) [1]+ lr*(t_min ^(v_(1_k)^1 )-t_min ^(v_(1_k ) ) );
end
Return v_(1_k );

CT.AVERAGE=(∑_(i=m_1)^(m_1+m_2-1)▒C_i )/m_2 	(7)
CT.AVGMAX=(∑_(i=m_1)^(〖m_1+m〗_2-1)▒〖MC_i 〗)/m_2 	(8)
CT.RATE=(∑_(i=m_1)^(〖m_1+m〗_2-1)▒C_i )/(∑_(i=m_1)^(〖m_1+m〗_2-1)▒〖MC_i 〗)	(9)

CT.AVERAGE means average common throughput for each test map with corresponding BTP_i (N,M)(m_1≤i<m_1+m_2), and CT.AVGMAX means maximum common throughput value for each throughput map corresponding to each test map, and CT.RATE means the rate between the sum of C_i and the sum of MC_i for all test maps. It also means the rate between CT.AVERAGE and CT.AVGMAX. We also define performance rate PR as (10) meaning how well our methodology is compared to the methodology used in the original paper, and the original paper in (10) means [2].

PR=((CT.AVERAGE of M_1 ))/((CT.AVERAGE of M_0 ) )
where M_1  is our methodology and
M_0  is the methodology in
original paper	(10)

CT.RATE can be larger than 1.0 because CT.AVGMAX means the average of largest value among the value at discrete blocks from corresponding TM_i, but CT.AVERAGE means the average of common throughput value with non-discrete HAP location.
Experimental Environment
The computer system information for our experiment is as the following. The operating system is Window 10 Pro 64bit (10.0, build 18363), system manufacturer is LG Electronics, the system model is 17ZD90N-VX5BK, the BIOS is C2ZE0160 X64, the processor is Intel® Core™ i5-1035G7 CPU @ 1.20GHz (8 CPUs), ~1.5GHz, and the memory is 16384MB RAM. The programming language is Python 3.7.4, and used NumPy [6], Tensorflow [7] and Keras as libraries. You can download the experiment code from https://github.com/WannaBeSuperteur/2020/tree/master/WPCN.
Experimental Results
Table I describes CT.RATE (%) and CT.AVERAGE values for our methodology and the methodology in the original paper. We used f_d=9.15×〖10〗^8,P_0=1.0,A_d=3.0,η=0.51,d_D=2.2,δ=20,σ=〖10〗^(-6) and β=A_d ((3∙〖10〗^8)/(4πf_d ))^(d_D ) with π=3.141592654 for the methodology in [2], and the algorithm to solve (20) in [2] is described in Algorithm 2. For our methodology, CT.RATE value increases when the number of WDs increases and decreases when the size of maps increases, and CT.AVERAGE decreases when both the number of WDs and the size of maps increases. For the methodology in the original paper, CT.RATE increases when the size of maps increases, but has no significant correlation with the number of WDs, and as of our methodology, CT.AVERAGE decreases when both the number of WDs and the size of maps increases. Table II shows the values of CT.AVGMAX
TABLE I
CT.RATE AND CT.AVERAGE VALUES OF OUR METHODOLOGY AND THE METHODOLOGY IN THE ORIGINAL PAPER
Our Methodology	CT.RATE (%)	CT.AVERAGE
Size / WDs	6 WDs	10 WDs	6 WDs	10 WDs
8x8	93.8477	98.6283	0.010175	0.006757
12x12	88.9782	91.1735	0.002626	0.001631
16x16	80.0318	89.6754	0.000796	0.000525
Methodology in the Original Paper	CT.RATE (%)	CT.AVERAGE
Size / WDs	6 WDs	10 WDs	6 WDs	10 WDs
8x8	65.5779	71.9627	0.007110	0.004930
12x12	89.1676	90.3931	0.002631	0.001617
16x16	100.6565	96.8606	0.001001	0.000568
The upper table is about our methodology and lower table represents CT.RATE and CT.AVERAGE about the methodology in the original paper. Size means the size of the board representing the environment of WPCN.


TABLE II
THE VALUE OF CT.AVGMAX AND PR
CT.AVGMAX	PR
Size / WDs	6 WDs	10 WDs	Size / WDs	6 WDs	10 WDs
8x8	0.010842	0.006851	8x8	1.431087	1.370548
12x12	0.002951	0.001789	12x12	0.997876	1.008633
16x16	0.000994	0.000586	16x16	0.795098	0.925819
The table on the left and on the right represents CT.AVGMAX and PR respectively for each option (Size and the number of WDs). There can be some errors for PR values because the values of CT.AVERAGE has less significant figures than 6.


 
Fig. 6. Line Chart Vestion of Table I and Table II. For the 4 tables in the left, upper 2 table shows the result of CT.RATE and CT.AVERAGE of our methodology and lower 2 table shows the result of them of the methodology in the original paper. The 2 tables in the right shows the values of CT.AVGMAX and PR, respectively.

 
Fig. 7. Comparison of CT.RATE(%) of our methodology and the methodology in the original paper. Our methodology shows better performance than the methodology in the original paper for size 8x8, but the two method shows nearly the same performance for size 12x12, and our methodology shows worse performance for size 16x16.

and PR for each size and number of WDs. CT.AVGMAX decreases when both the number of WDs and the size of maps increases and PR decreases when the size of maps increases, but has no significant correlation with the number of WDs. For smaller sizes, our methodology shows significantly better performance (PR>1) than the methodology in the original paper, but for 12x12 size, these two methods show almost the same performance. (PR≈1), and for 16x16 size, our methodology shows worse performance. (PR<1) Fig. 6. is the line chart representation of Table I and Table II, and Fig. 7. is the bar chart for comparison of our methodology and the methodology in the original paper.
 DISCUSSION
Our method shows higher CT.RATE for smaller maps and the methodology in the original paper shows higher CT.RATE for larger maps. The reason for the former is that common throughput is usually depending on the WDs near the boundary of the environment, because it enlarges the minimum value of maximum possible distance between the HAP and each WD, and the influence on the learning, of the WDs near the boundary, decreases for larger maps. The reason for the latter is that the location of WDs can be ‘skewed’ for smaller maps, and the discriticity of location of them has more influence than larger maps, so the methodology in the original paper is not so accurate.
CONCLUSION
We showed that our deep learning-based method shows better performance than the mathematical method in the original paper [2] when the size is smaller than 12x12. Although our method may show worse performance if the size is larger than 12x12, our approach to find the optimal placement and time allocation for HAP using deep learning is meaningful because there is no attempt to apply deep learning to this problem yet. There are some limits for our study. First, our study has an advantageous point for our method that it uses only 1 HAP which is fitted to the experimental environment, but the method in the original paper may and commonly uses more than 1 HAPs. Second, we studied with just a few conditions, 3 options for map size and 2 options for the number of WDs. So some future study should be done for many options for map size and the number of WDs, and additionally the number of HAPs.
References
Suzhi Bi, Yong Zeng, and Rui Zhang, “Wireless Powered Communication Networks: An Overview”, IEEE, available online at https://arXiv:1508.06366.
Suzhi Bi, Member, IEEE, and Rui Zhang, “Placement Optimization of Energy and Information Access Points in Wireless Powered Communication Networks”, IEEE Transactions on wireless communications, VOL. 15, NO. 3, MARCH 2016.
Hyungsik Ju and Rui Zhang, “Throughput Maximization in Wireless Powered Communication Networks”, available online at https://arXiv:1304.7886v4.
Diederik P. Kingma, Jimmy Lei Ba, “ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION”, ICLR 2015, available online at https://arXiv:1412.6980.
Saad ALBA WI, and Saad AL-ZA WI, “Understanding of a Convolutional Neural Network”, ICET 2017, available online at https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8308186.
IEEE, “The NumPy Array: A Structure for Efficient Numerical Computation”, Scientific Python, available online at https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5725236.
Mart´ın Abadi, Paul Barham, Jianmin Chen et al, “TensorFlow: A system for large-scale machine learning”, Google Brain, available online at https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf.

